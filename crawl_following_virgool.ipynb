{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "from unidecode import unidecode\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_following(url):\n",
    "    \n",
    "    break_levels = 0\n",
    "    \n",
    "    global following\n",
    "    following = []\n",
    "    \n",
    "    #Number of Pages to consider for each User --> here is 99 pages in Maximum\n",
    "    #Each page in virgool contains 40 users data\n",
    "    for i in range(1,100):\n",
    "        \n",
    "        user_following_link = url+\"/following/{}\".format(i)\n",
    "        driver.get(user_following_link)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        #Sometimes the page is empty or an error page pops-up\n",
    "        #In that case we need to stop and go to another User\n",
    "        if not soup.find_all('div',{'class':'listview-item author'}):\n",
    "            break\n",
    "        \n",
    "        #Goes through 40 users that the source user follows and picks their information\n",
    "        for user in soup.find_all('div',{'class':'listview-item author'}):\n",
    "            user_identity = user.find('a',{'class':'item--title'}, href=True)\n",
    "            user_desc = user.find('div',{'class': 'item--descreption'})\n",
    "            \n",
    "            #If the information is repetitive, it shows that the bot is at the last page and should break\n",
    "            if ([url[20:],user_identity['href'][20:],user_identity.text,user_identity['href'],user_desc.text[1:-1]]) in following:\n",
    "                break_levels = 1\n",
    "                break\n",
    "            else:\n",
    "                following.append([url[20:],user_identity['href'][20:],user_identity.text,user_identity['href'],user_desc.text[1:-1]])\n",
    "\n",
    "        #time.sleep(2)\n",
    "        if break_levels == 1:\n",
    "            break\n",
    "\n",
    "        \n",
    "    return following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A simple function to add each users data to a CSV file\n",
    "def write_to_csv(file_path, header, data):\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    with open(file_path, 'a', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        if file_exists != True:\n",
    "            writer.writerow(i for i in header)\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get User Followings\n",
    "output_dir = 'Desktop'\n",
    "user_following_csv_file_path = os.path.join(output_dir, 'following.csv')\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver')\n",
    "\n",
    "#How many hops to go?\n",
    "hops = 5\n",
    "\n",
    "#set a user to start crawling from\n",
    "nodes_list = {0:['https://virgool.io/@saeed.choobani']}\n",
    "\n",
    "#create a remove set to prevent crawling replicated users\n",
    "removeset = set()\n",
    "\n",
    "#In each hope we create a dictionary of the next hop's links\n",
    "for k in range(hops):\n",
    "    \n",
    "    nodes_list.update({k+1:[]})\n",
    "    \n",
    "    for node in nodes_list[k]:\n",
    "\n",
    "        \n",
    "        get_user_following(node)\n",
    "\n",
    "        \n",
    "        for j in following:\n",
    "            header = ['source', 'target', 'target_name', 'target_url', 'target_desc']\n",
    "            write_to_csv(user_following_csv_file_path, header, j)\n",
    "        \n",
    "        \n",
    "        following = np.array(following)\n",
    "        try:\n",
    "            for a in following[:,3].tolist():\n",
    "                nodes_list[k+1].append(a)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    \n",
    "    removeset.update(nodes_list[k])\n",
    "    nodes_list[k+1] = [x for x in nodes_list[k+1] if x not in removeset]\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and Extra function in case you want to gather data from user page\n",
    "def get_user_info(url):\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    for i in soup.find_all('div', {'class':'follow-count js-follow-section-count'}):\n",
    "        follow_count = i.find_all('span')\n",
    "\n",
    "    following_count = int(unidecode(u\"{}\".format(follow_count[1].text)))\n",
    "    follower_count = int(unidecode(u\"{}\".format(follow_count[0].text)))\n",
    "    user_identity = soup.find('a', {'class':'module--name'}, href=True)\n",
    "    user_fullname = user_identity.text[1:-1]\n",
    "    user_url = user_identity['href']\n",
    "    username = user_identity['href'][20:]\n",
    "    user_desc = soup.find('p', {'class':'module--bio js--linkify'}).text\n",
    "\n",
    "\n",
    "    return (username,user_fullname,user_desc, following_count, follower_count)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
